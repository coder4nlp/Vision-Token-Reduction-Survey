# 1 ​​研究背景​​
* **问题**：这篇论文旨在解决大型视觉-语言模型（LVLMs）在处理高分辨率图像和多图像/视频输入时，视觉token数量显著增加，导致计算成本高昂的问题。
* **难点**：直接减少视觉token数量可能会丢失重要信息，影响模型的最终响应质量。现有的视觉token缩减方法主要分为两类：文本无关的剪枝方法和文本感知的剪枝方法，但它们的性能往往受限于单阶段设计和缺乏对视觉token处理过程的系统理解。
* **相关工作**：现有工作主要包括基于视觉编码器输出层的`[CLS]`注意力分数进行剪枝的方法，以及在语言解码器早期层进行文本感知剪枝的方法。然而，这些方法在处理复杂任务时可能会忽略局部细节或引入位置偏差。
# 2 ​​研究方法​​
提出了`VScan`，一个两阶段的训练免费视觉token缩减框架，通过在视觉编码和语言解码阶段逐步剪除无关紧要的token来提高LVLM的效率。

在视觉编码阶段，VScan采用全局和局部扫描策略，结合token合并技术，保留语义重要且空间多样的token。具体来说，全局扫描通过`[CLS]`注意力选择全局重要token，局部扫描则在浅层通过窗口划分选择局部重要token，最终通过相似性合并策略减少信息损失。

在语言解码阶段，VScan在中层引入剪枝策略，避免早期层的位置偏差和深层的语义干扰，保留关键的跨模态交互。具体实现中，VScan在中层计算视觉token与最后指令token的注意力，选择相关性最高的token进行保留。
# 3 ​​实验设计​​
在LLaVA-1.5、LLaVA-NeXT、Qwen-2.5-VL和Video-LLaVA四个LVLMs上进行了广泛的实验，评估了VScan在16个图像和视频理解基准上的性能。

实验结果表明，VScan在保持高性能的同时显著减少了计算成本。例如，在LLaVA-NeXT-7B上，VScan实现了2.91倍的预填充加速和10倍的FLOPs减少，同时保留了95.4%的原始性能。
# 4 ​​结果与分析​​
在LLaVA-1.5-7B上，VScan在仅保留128和192个token的情况下，性能几乎没有下降，平均性能下降仅为1.0%和1.2%。在更高的缩减率下，VScan仍然表现出色，平均性能下降仅为3.3%，优于现有的VisionZip方法4.0%。
在LLaVA-NeXT-7B上，VScan在保留320个token的情况下，性能达到原始模型的95.4%，在多个基准测试中表现优异。
在Qwen-2.5-VL上，VScan在不同LLM规模下均表现出色，特别是在低token预算下，VScan的性能优于FastV和PyramidDrop方法。
# 5 ​​总体结论​​
VScan通过两阶段的视觉token缩减策略，显著提高了LVLM的推理效率，同时保持了强大的性能。实验结果表明，VScan在多个基准测试中均优于现有的最先进方法，展示了其在实际应用中的潜力。