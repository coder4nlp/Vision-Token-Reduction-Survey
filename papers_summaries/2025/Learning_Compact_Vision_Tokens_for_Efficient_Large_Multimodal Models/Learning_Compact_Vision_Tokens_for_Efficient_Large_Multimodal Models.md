# 研究问题
本文旨在解决大型多模态模型（LMMs）在计算效率方面的挑战，特别是由于大语言模型（LLMs）的高成本和长视觉token序列的二次复杂性导致的计算瓶颈。

# 研究方法
本文提出了一种空间token融合（Spatial Token Fusion, STF）方法，通过将空间相邻的token融合为一个token来缩短视觉token序列，从而加速推理过程。具体方法如下：

* ​空间Token融合（STF）​​：将空间相邻的token融合为一个token，以减少视觉token序列的长度。与简单的平均池化不同，STF模块通过卷积操作在滑动窗口中融合相邻的token，保留更多的信息。

* ​​多块Token融合（Multi-Block Token Fusion, MBTF）​​：为了补充多粒度特征，MBTF模块从视觉编码器的多个块中提取token，并在通道维度上进行融合。MBTF模块通过两个连续的1x1卷积和GeLU激活函数来融合特征，逐步减少通道维度。
* ​​优化​​：采用LLaVA的训练方案，通过最大化生成目标响应的概率来优化模型参数。优化过程包括特征对齐预训练和端到端微调两个阶段。
# 实验设计
实验在8个流行的视觉-语言基准测试上进行，包括GQA、ScienceQA、VQAv2、VisWiz、TextVQA、POPE、MMBench和MMBench-CN。实验设置如下：

**​模型架构​**​：使用CLIP ViT-L/14作为视觉编码器，输入图像分辨率为336x336，LLM骨干为Vicuna-1.5-7B。

**​​数据准备**​​：在过滤后的CC-595K子集上进行预训练，学习率为1e-3，批量大小为256，训练1个epoch；在LLaVA-Instruct-158K数据集上进行微调，学习率为2e-5，批量大小为128，训练1个epoch。

**​​评估指标**​​：在8个基准测试上评估模型的性能，计算平均得分。
## 结果与分析
实验结果表明，本文方法在仅使用25%视觉token的情况下，性能优于基线模型LLaVA-1.5-7B，甚至在某些基准测试上表现更好。具体结果如下：

​​平均性能​​：本文方法在8个基准测试上的平均得分为66.3%，优于LLaVA-1.5-7B的65.5%。
​​各基准测试表现​​：在SQA、MMBench、VQAv2和VisWiz等基准测试上，本文方法表现尤为突出，分别比基线模型高出1.4%、1.9%、0.6%和3.3%。
## 总体结论
本文提出了一种新的token融合方法，通过空间Token融合和多块Token融合模块，有效减少了视觉token序列的长度，同时保留了图像的关键信息。实验结果表明，本文方法在显著加速推理过程的同时，保持了甚至提升了多模态推理能力。未来的工作将探索更高效的策略，以进一步减少token融合过程中的信息损失。