# 研究问题
这篇论文旨在解决视觉-语言模型（VLMs）在推理过程中计算资源需求高的问题，特别是由于视觉输入token数量庞大导致的计算和内存开销。

# 研究方法
论文提出了一种名为TopV的兼容性token剪枝方法，用于在推理时优化VLM的性能。TopV通过优化问题来识别和剪枝不重要的视觉token，而不是依赖于注意力分数。具体方法包括：

* ​​Token重要性公式化​​：将token剪枝问题公式化为一个优化问题，目标是通过最小化运输成本来识别对后续层贡献最大的源token。
* ​​目标token的确定​​：选择Post-LN层的输出作为目标token，以确保目标token能够反映源token与目标token之间的显著差异，同时捕捉Attention模块的主要功能。
* ​​视觉感知成本函数​​：引入了特征相似性、相对空间距离和绝对中心距离三个因素来构建成本函数，以增强token选择的有效性。
* ​​Sinkhorn算法求解​​：使用Sinkhorn算法迭代求解优化问题，得到贡献矩阵，并根据贡献分数剪枝不重要的token。
* ​​Token恢复策略​​：在剪枝后，采用均匀采样方法恢复部分被剪枝的token，以维持视觉token的结构完整性。
# 实验设计
实验在多个VLM模型（如LLaVA和InternVL2）上进行，评估了TopV在不同任务（如图像描述、视觉问答、OCR等）中的性能。实验设置包括：

* ​​模型设置​​：主要在LLaVA-v1.5-7B和InternVL2-2B上进行实验，同时也评估了更大规模的模型如LLaVA-13B和InternVL2-26B。
* ​​评估数据集​​：包括AI2D、SQA_image、MMMU、MMBench、POPE、Npcaps、OK-VQA和OCRBench等。
* ​​实现细节​​：实验在A6000 GPU上进行LLaVA-v1.5-7B和InternVL2-2B的实验，在A100 GPU上进行LLaVA-v1.5-13B和InternVL2-26B的实验。

# 结果与分析
实验结果表明，TopV在多个任务中显著优于现有的token剪枝方法：

* ​​性能提升​​：在InternVL2模型上，TopV在保持几乎无精度损失的情况下，将视觉FLOPs减少了47%，动态内存使用减少了61%，推理效率提高了2.1倍。对于LLaVA模型，TopV在减少50%视觉FLOPs的情况下，性能提高了0.39%，动态内存使用减少了49%，推理效率提高了1.68倍。
* ​​内存使用​​：TopV与KV cache兼容，减少了动态内存使用，显著提高了内存效率。
* ​​任务表现​​：在OCR任务中，TopV成功识别了“Jump”一词，而Baseline和FastV方法未能正确识别。
# 总体结论
TopV通过优化问题公式化token剪枝，显著提高了VLM的推理效率和内存使用效率。该方法无需额外训练或微调，兼容FlashAttention和KV cache，适用于资源受限的环境。实验结果验证了TopV在多个任务中的有效性和效率。