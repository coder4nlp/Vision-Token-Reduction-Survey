# ​​研究背景​​
**问题**：视觉指令调优旨在使大型语言模型（LLMs）能够理解视觉世界，其核心挑战在于建立有效的视觉到语言的投影。然而，现有的方法在准确性和效率之间难以取得平衡。

**难点**：视觉标记的数量显著增加了LLMs的计算负担。例如，CLIP ViT-L/336px模型将一张$672 \times 1008$的图像编码为$34\times 56$个标记，这对计算资源提出了很高的要求。

**相关工作**：现有方法主要分为两类：融合和选择。融合方法通过预定义的合并操作或可学习的模块将视觉标记压缩为固定数量的表示，但可能损失细粒度信息或需要大量训练数据。选择方法则通过任务无关的视觉线索选择重要标记，但往往忽略指令相关性。
# ​​研究方法​​
提出了LLaVA-Meteor模型，采用自上而下的压缩范式，通过全局融合和本地选择来压缩视觉标记。

引入了Flash Global Fusion (FGF)模块，利用状态空间模型（SSMs）进行高效的全局上下文传播，并通过局部到单扫描策略增强局部空间理解。具体来说，FGF模块在处理每个标记时，首先构建一个3x3的局部窗口，聚合局部上下文信息，然后通过SSMs进行全局上下文传播。

提出了Visual-Native Selection (VNS)模块，通过视觉专家（类标记）和本地专家（指令标记）的双重评估机制，综合考虑视觉显著性和指令相关性来选择最重要的标记。视觉专家通过计算每个标记与类标记的注意力权重来评估视觉重要性，而本地专家则通过计算每个标记与指令标记的相似度来评估指令相关性。
# ​​实验设计​​
在12个广泛使用的基准测试上评估LLaVA-Meteor，包括文本导向的VQA（如TextVQA、DocVQA）、一般VQA（如GQA、VQAv2）和综合评估（如MMB、MMMU）。

实验结果表明，LLaVA-Meteor在使用显著减少的视觉标记（减少75%-95%）的情况下，仍然在大多数任务上取得了与或优于现有方法的表现。例如，在TextVQA上，LLaVA-Meteor在使用144个标记时达到了69.9的得分，而在使用64个标记时仍保持了58.6的得分，仅下降了-0.4。
​​结果与分析​​：

具体来说，在TextVQA上，LLaVA-Meteor在使用144个标记时达到了69.9的得分，而在使用64个标记时仍保持了58.6的得分，仅下降了-0.4。在GQA上，LLaVA-Meteor在使用144个标记时达到了82.4的得分，而在使用64个标记时仍保持了64.6的得分，下降了-0.8。

这些结果表明，LLaVA-Meteor在减少视觉标记数量的同时，仍然能够保持较高的任务性能，验证了其压缩方法的有效性。
# ​​总体结论​​
提出了Top-Down Compression框架，通过全局融合和专家引导的标记选择，有效地平衡了性能和效率。
LLaVA-Meteor在多个基准测试中表现出色，显著减少了视觉标记的使用，同时保持了强大的任务性能，适用于资源受限的实际应用场景。

这篇论文通过引入新的压缩方法和双重评估机制，为视觉指令调优提供了一种高效的解决方案，具有重要的理论和实际意义。